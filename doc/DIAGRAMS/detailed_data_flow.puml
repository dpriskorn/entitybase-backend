@startuml detailed_data_flow

title Complete Data Flow: Entity Creation â†’ RDF Change Event

' Entry Points
package "REST API Layer" #LightBlue {
    rectangle "Entity Endpoints" as API
}

' Processing
package "Entity Processing" #LightGreen {
    rectangle "Request Validation" as VALIDATE
    rectangle "Idempotency Check" as IDEMPOTENT
    rectangle "Entity Processing" as PROCESS
    rectangle "Hashing and Deduplication" as HASH
}

' Storage
package "Storage Layer" #LightYellow {
    database "Vitess Repository" as DB
    storage "S3 Client" as S3
}

' Event Publishing Step 1
queue "Kafka entitybase.entity_change" as KAFKA1

' Entity Diff Worker
package "Entity Diff Worker" #Lavender {
    rectangle "Entity Diff Worker" as WORKER
    rectangle "RDF Serializer" as RDF_CONV
    rectangle "RDF Canonicalizer URDNA2015" as CANONICAL
    rectangle "Diff Computation" as DIFF
}

' Event Publishing Step 2
queue "Kafka wikibase.entity_diff" as KAFKA2

' Internal Consumer
package "Internal Consumers" #Coral {
    rectangle "Watchlist Consumer" as WATCHLIST
}

' Data Flow Connections
API --> VALIDATE : 1. HTTP Request
VALIDATE --> IDEMPOTENT : 2. Validated
IDEMPOTENT --> PROCESS : 3. Unique request
PROCESS --> HASH : 4. Processed data
HASH --> DB : 5a. Revision metadata
HASH --> S3 : 5b. Full snapshot

DB --> KAFKA1 : 6a. Trigger
S3 --> KAFKA1 : 6b. Trigger

KAFKA1 --> WORKER : 7. EntityChangeEvent
WORKER --> RDF_CONV : 8. JSON to Turtle
RDF_CONV --> CANONICAL : 9. Canonicalize
CANONICAL --> DIFF : 10. Normalized triples
DIFF --> KAFKA2 : 11. RDFChangeEvent

KAFKA1 --> WATCHLIST : 12. EntityChangeEvent

note bottom of KAFKA2
  External consumers (not in this codebase):
  QLever
  Search indexers
  Analytics pipelines
  Mirror services
end note

@enduml
